# üìä TensorBoard Integration - DRL-Flatten-Image

## M√©tricas Implementadas

Basado en el paper P√©rez-Gil et al. (2022), el sistema de logging con TensorBoard incluye:

### 1. M√©tricas por Episodio (`Episode/*`)
- **Total_Reward**: Reward acumulado en el episodio
- **Length**: N√∫mero de steps en el episodio
- **Avg_Reward_Per_Step**: Reward promedio por step
- **Collision**: Si hubo colisi√≥n (binario: 0 o 1)
- **Lane_Invasion**: Si hubo invasi√≥n de carril (binario: 0 o 1)

### 2. M√©tricas Acumuladas (`Cumulative/*`)
- **Collision_Rate**: Tasa de colisiones a lo largo del entrenamiento
- **Lane_Invasion_Rate**: Tasa de invasiones de carril

### 3. M√©tricas de Entrenamiento (`Training/*`)
- **Epsilon**: Tasa de exploraci√≥n (epsilon-greedy para DQN)
- **Loss**: Loss de la red neuronal (si disponible)

### 4. Estado del Veh√≠culo (`State/*`)
Seg√∫n el paper (dft = (vt, dt, œÜt)):
- **Velocity_vt**: Velocidad del veh√≠culo (m/s)
- **Distance_dt**: Distancia al centro del carril (m)
- **Angle_phi_t**: √Ångulo con respecto al carril (grados)

### 5. Acciones de Control (`Action/*`)
- **Throttle**: Aceleraci√≥n [0, 1]
- **Steering**: Direcci√≥n [-1, 1]
- **Brake**: Frenado [0, 1]

### 6. Reward por Step (`Step/*`)
- **Reward**: Reward instant√°neo en cada step

### 7. Promedios M√≥viles (`Running_Avg_N/*`)
- **Reward**: Promedio m√≥vil de rewards (ventana configurable)
- **Length**: Promedio m√≥vil de longitudes de episodio

### 8. Mejor Modelo (`Best/*`)
- **Episode**: Episodio del mejor modelo
- **Reward**: Mejor reward alcanzado

## Uso

### Iniciar TensorBoard

```bash
# Desde el directorio del proyecto
tensorboard --logdir=runs

# Luego abrir en navegador: http://localhost:6006
```

### Configuraci√≥n en C√≥digo

El logger se inicializa autom√°ticamente en `main.py`:

```python
tb_logger = TensorBoardLogger(log_dir='runs', experiment_name=experiment_name)
```

### Hiperpar√°metros Logged

Todos los hiperpar√°metros del experimento se guardan autom√°ticamente:
- Algoritmo (DQN/DDPG)
- Arquitectura (DRL-Flatten-Image)
- Buffer size, batch size, gamma, learning rate
- Epsilon parameters
- State/action sizes
- Image resolution

## Visualizaciones Disponibles

### 1. Scalars
- Gr√°ficas de todas las m√©tricas num√©ricas a lo largo del entrenamiento
- Permite comparar m√∫ltiples runs

### 2. Histograms
- Distribuci√≥n de valores (si se loggean histogramas)

### 3. HParams
- Comparaci√≥n de hiperpar√°metros entre diferentes experimentos
- B√∫squeda de mejores configuraciones

## Comparaci√≥n con Paper

El paper P√©rez-Gil et al. (2022) usa estas m√©tricas principales:
1. ‚úÖ **Total reward** por episodio
2. ‚úÖ **Episode length** (n√∫mero de steps)
3. ‚úÖ **Driving features** (vt, dt, œÜt)
4. ‚úÖ **Collision/Lane invasion** events
5. ‚úÖ **RMSE** en validaci√≥n (calculado offline)

Todas estas m√©tricas est√°n implementadas y se visualizan en TensorBoard.

## Frecuencia de Logging

Para evitar overhead excesivo:
- **M√©tricas por step**: Cada 10 steps (configurable)
- **M√©tricas por episodio**: Cada episodio
- **Checkpoints**: Cada 50 episodios (configurable)

## Ejemplo de Salida

```
üìä TensorBoard logger initialized: runs/DQN_Flatten_20251019_143025
   Run: tensorboard --logdir=runs
```

## Notas

- Los logs se guardan en `runs/<experiment_name>/`
- Cada experimento tiene un timestamp √∫nico
- Los hiperpar√°metros se guardan para reproducibilidad
- Compatible con m√∫ltiples runs paralelos

## Referencias

Paper: P√©rez-Gil et al. (2022) "Deep reinforcement learning based control for Autonomous Vehicles in CARLA"
- DOI: 10.1007/s11042-021-11437-3
- Secci√≥n 6: Results (m√©tricas de evaluaci√≥n)
