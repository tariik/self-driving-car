# ğŸ“‹ ConfiguraciÃ³n EXACTA: DRL-Flatten-Image del Paper

**Paper**: PÃ©rez-Gil et al. (2022) - "Deep reinforcement learning based control for Autonomous Vehicles in CARLA"

---

## ğŸ¯ CONFIGURACIÃ“N EXACTA DEL PAPER

### **Estado (State) - EcuaciÃ³n (21)**

```
S = ([Pt0, Pt1, Pt2, ...Pt120], Ï†t, dt)
```

**Componentes:**
1. **Imagen B/W**: 640x480 â†’ redimensionada a **11x11** â†’ aplanada a **121 valores**
2. **Ï†t**: Ãngulo al carril (rad)
3. **dt**: Distancia al centro del carril (m)

**Total**: 121 + 1 + 1 = **123 dimensiones**

### **AcciÃ³n (Action)**

- **DQN**: 27 acciones discretas (Tabla 1)
  - Steering: -1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1 (9 valores)
  - Throttle: 0, 0.5, 1 (3 valores)
  - Total: 9 Ã— 3 = 27 combinaciones

- **DDPG**: Acciones continuas
  - Steering: [-1, 1]
  - Throttle: [0, 1]

### **Reward Function - Ecuaciones (18-19)**

```python
# EcuaciÃ³n (18)
R = -200  if collision or lane_change or roadway_departure

# EcuaciÃ³n (19)
R = Î£ |vtÂ·cos(Ï†t)| - |vtÂ·sin(Ï†t)| - |vt|Â·|dt|  if car in lane
```

### **Red Neuronal**

**Arquitectura (del paper - lÃ­nea 560):**
- Input: 123 dimensiones (121 imagen + Ï†t + dt)
- 2 Fully-Connected Layers (tamaÃ±o no especificado exactamente)
- Output: 27 acciones (DQN) o 2 acciones (DDPG)

### **Entrenamiento**

| ParÃ¡metro | DQN-Flatten-Image | DDPG-Flatten-Image |
|-----------|-------------------|---------------------|
| **Total Episodes** | 20,000 | 500 |
| **Best Episode** | 16,500 | 50 |
| **Mapa** | Town01 | Town01 |
| **Rutas** | Aleatorias (training) | Aleatorias (training) |
| **Ruta validaciÃ³n** | Fija ~180m | Fija ~180m |

### **Condiciones de TÃ©rmino de Episodio**

SegÃºn lÃ­neas 714-717 del paper:

```python
done = True if:
    - collision_sensor activated
    - lane_invasor activated
    - max episodes reached
```

### **Hardware (lÃ­nea 726-727)**

```
CPU: Intel Core i7-9700k
RAM: 32GB
GPU: NVIDIA GeForce RTX 2080 Ti (11GB VRAM)
```

---

## âš ï¸ DIFERENCIAS CON TU CÃ“DIGO ACTUAL

| Aspecto | Paper | Tu CÃ³digo Actual | Estado |
|---------|-------|------------------|--------|
| **TamaÃ±o imagen** | 11x11 (121 valores) | 84x84 (7,056 valores/frame) | âŒ DIFERENTE |
| **Frame stack** | 1 frame | 4 frames | âŒ DIFERENTE |
| **Estado total** | 123 dim (121 + Ï†t + dt) | 28,224 dim (84Ã—84Ã—4) | âŒ DIFERENTE |
| **Ï†t, dt en estado** | âœ… SÃ incluidos | âŒ NO incluidos | âŒ DIFERENTE |
| **Reward -200** | âœ… | âœ… | âœ… CORRECTO |
| **Episode ends** | âœ… collision + lane | âœ… | âœ… CORRECTO |
| **Rutas aleatorias** | âœ… | âœ… | âœ… CORRECTO |
| **Mapa Town01** | âœ… | âœ… | âœ… CORRECTO |
| **27 acciones** | âœ… | âœ… | âœ… CORRECTO |

---

## ğŸ”§ CAMBIOS NECESARIOS

### 1. **Cambiar tamaÃ±o de imagen: 84x84 â†’ 11x11**

```python
# En config de cÃ¡mara
"image_size_x": "11",  # Cambiar de 84
"image_size_y": "11",  # Cambiar de 84
"size": 11  # Para observation space
```

### 2. **Eliminar frame stacking (usar solo 1 frame)**

```python
# En base_env.py
"framestack": 1,  # Cambiar de 4 a 1
```

### 3. **Agregar Ï†t y dt al estado**

```python
# En base_env.py - get_observation()
# Concatenar: [imagen_11x11_flatten] + [Ï†t] + [dt]
driving_features = self.get_driving_features(core)
image_flat = image.flatten()  # 121 valores
state = np.concatenate([image_flat, [driving_features[2]], [driving_features[1]]])
# Total: 121 + 1 + 1 = 123 dimensiones
```

### 4. **Actualizar red neuronal**

```python
# En drl_flatten_agent.py
class QNetwork(nn.Module):
    def __init__(self, state_size=123, action_size=27, seed=0):
        # state_size = 123 (121 imagen + 1 Ï†t + 1 dt)
        # fc1: 123 â†’ 64
        # fc2: 64 â†’ 32
        # fc3: 32 â†’ 27
```

---

## ğŸ“Š RESULTADOS ESPERADOS (segÃºn paper)

### **DQN-Flatten-Image:**
- Training episodes: 20,000
- Best episode: 16,500
- RMSE: ~0.0968m (Tabla 3, lÃ­nea 854)
- Tiempo promedio: ~11.8s en ruta de 180m

### **DDPG-Flatten-Image:**
- Training episodes: 500
- Best episode: 50
- RMSE: ~0.0956m (Tabla 3, lÃ­nea 858)
- Tiempo promedio: ~11.0s en ruta de 180m
- **50x mÃ¡s rÃ¡pido que DQN** (50 vs 16,500 episodios)

---

## âœ… PLAN DE ACCIÃ“N

1. âœ… Cambiar tamaÃ±o imagen a 11x11
2. âœ… Cambiar framestack a 1
3. âœ… Agregar Ï†t y dt al estado
4. âœ… Actualizar observation_space a 123
5. âœ… Actualizar red neuronal (input 123)
6. âœ… Verificar 27 acciones discretas
7. âœ… Confirmar Town01
8. âœ… Confirmar rutas aleatorias
9. âœ… Entrenar y comparar con resultados del paper

Â¿Quieres que implemente estos cambios ahora? ğŸš€
