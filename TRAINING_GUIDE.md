# ğŸš— DRL-Flatten-Image: Entrenamiento y EvaluaciÃ³n

## ğŸ“š ImplementaciÃ³n del Paper
**"Deep reinforcement learning based control for Autonomous Vehicles in CARLA"**  
PÃ©rez-Gil et al. (2022) - SecciÃ³n 5.1: DRL-Flatten-Image Agent

## ğŸ¯ Arquitectura Implementada

### Estado (EcuaciÃ³n 21)
```
S = ([Pt0, Pt1, ..., Pt120], Ï†t, dt)
```
- **Imagen**: 11Ã—11 pixels aplanada = 121 valores
- **Ï†t**: Ãngulo al carril (radianes)
- **dt**: Distancia al centro (metros)
- **Total**: 123 dimensiones

### Red Neuronal (DQN)
```
Input (123) â†’ FC1 (64, ReLU) â†’ FC2 (32, ReLU) â†’ Output (27 acciones)
```

### Acciones Discretas (Tabla 1 del paper)
- **Steering**: 9 posiciones [-1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1]
- **Throttle**: 3 posiciones [0, 0.5, 1]
- **Total**: 9 Ã— 3 = 27 combinaciones

## ğŸš€ Uso

### 1ï¸âƒ£ Entrenamiento
```bash
# Entrenar modelo nuevo (500 episodios por defecto)
python src/main.py

# Checkpoints se guardan cada 50 episodios en:
# checkpoints/drl_flatten_episode_50.pth
# checkpoints/drl_flatten_episode_100.pth
# ...
# checkpoints/drl_flatten_final.pth
```

**HiperparÃ¡metros:**
- Episodios: 500 (paper: DDPG necesita ~500, DQN ~20,000)
- Steps por episodio: 500
- Learning rate: 5e-4
- Batch size: 32
- Gamma: 0.99
- Replay buffer: 100,000

### 2ï¸âƒ£ EvaluaciÃ³n
```bash
# Evaluar Ãºltimo checkpoint (20 iteraciones)
python evaluate_agent.py

# Evaluar checkpoint especÃ­fico
python evaluate_agent.py --checkpoint checkpoints/drl_flatten_episode_200.pth

# Evaluar con mÃ¡s iteraciones
python evaluate_agent.py --iterations 50

# Ver guÃ­a completa
cat EVALUATION_GUIDE.md
```

**MÃ©tricas calculadas:**
- RMSE (Root Mean Square Error)
- Error mÃ¡ximo
- Tasa de Ã©xito
- Tiempo promedio
- ComparaciÃ³n con paper

### 3ï¸âƒ£ Monitoreo Durante Training

**Console output:**
```
Episode 1/500
   âœ“ Episode finished at step 245: Total reward = 123.45
   ğŸ† NEW BEST! Episode 1: Reward = 123.45

Episodes 1-10 Summary:
   Avg Reward: 98.32
   Avg Length: 234.5 steps
   Best so far: Episode 3 with 145.67

ğŸ’¾ Checkpoint saved: checkpoints/drl_flatten_episode_50.pth
ğŸ“Š Avg reward (last 50): 156.78
```

## ğŸ“Š Resultados Esperados (Paper - Tabla 3)

| MÃ©todo                 | RMSE (m) | Training Episodes |
|------------------------|----------|-------------------|
| LQR (baseline)         | 0.06     | N/A               |
| DQN-Flatten-Image      | 0.08     | ~16,500           |
| DDPG-Flatten-Image     | 0.07     | ~50               |

**Tu objetivo**: RMSE < 0.10 m (excelente), < 0.15 m (bueno)

## ğŸ› ï¸ ConfiguraciÃ³n del Entorno

### Sensores (EcuaciÃ³n 21)
- **CÃ¡mara RGB**: 640Ã—480 â†’ resize a 11Ã—11 â†’ grayscale â†’ normalizada [-1,1]
- **PosiciÃ³n**: x=2.0m adelante, z=1.2m altura
- **OrientaciÃ³n**: pitch=-25Â° (mirando a la carretera)
- **Driving features**: Ï†t (Ã¡ngulo) y dt (distancia) desde CARLA API

### Mapa y Rutas
- **Mapa**: Town01 (del paper)
- **Clima**: ClearNoon
- **Rutas**: Aleatorias en cada episodio (A* planner)
- **VisualizaciÃ³n**: Waypoints verdes a 5m altura

### FunciÃ³n de Recompensa (Paper)
```python
R = -200  # si colisiÃ³n o invasiÃ³n de carril
R = Î£ |vtÂ·cos(Ï†t)| - |vtÂ·sin(Ï†t)| - |vt|Â·|dt|  # si en carril
R = +100  # si meta alcanzada
```

## ğŸ“ Estructura de Archivos

```
self-driving-car/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                    # Training loop
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â””â”€â”€ drl_flatten_agent.py   # DQN implementation
â”‚   â”œâ”€â”€ env/
â”‚   â”‚   â”œâ”€â”€ base_env.py            # Environment base
â”‚   â”‚   â””â”€â”€ carla_env.py           # CARLA wrapper
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ video_recorder.py      # Video recording
â”‚       â””â”€â”€ display_manager.py     # Visualization
â”œâ”€â”€ evaluate_agent.py              # Evaluation script
â”œâ”€â”€ EVALUATION_GUIDE.md            # Evaluation guide
â”œâ”€â”€ checkpoints/                   # Saved models
â”œâ”€â”€ evaluation_output/             # Evaluation videos
â””â”€â”€ render_output/                 # Training videos (disabled)
```

## ğŸ® Controles

### Durante Training
- **ESC** o **Q**: Salir
- **Ventana**: Muestra vista chase-cam detrÃ¡s del vehÃ­culo

### Durante Evaluation
- **ESC** o **Q**: Salir y guardar resultados parciales

## ğŸ› Troubleshooting

### CARLA no estÃ¡ corriendo
```bash
./launch_carla.sh
# Esperar a "Listening on port 3000"
```

### GPU Out of Memory
El cÃ³digo automÃ¡ticamente fallback a CPU. Para forzar CPU:
```python
device = torch.device("cpu")
```

### Display no funciona (headless)
```bash
python src/main.py  # Training no necesita display
python evaluate_agent.py --no-display  # Evaluation headless
```

### Training muy lento
1. Desactiva renders (ya desactivado por defecto)
2. Reduce batch size: `BATCH_SIZE = 16`
3. Reduce episodios: `MAX_EPISODES = 100`

## ğŸ“ˆ Mejoras Sugeridas

### Para mejor RMSE
1. MÃ¡s episodios (paper DQN usa ~20K)
2. Ajustar learning rate
3. Usar DDPG en vez de DQN (mejor para control continuo)

### Para mejor velocidad
1. AÃ±adir tÃ©rmino de velocidad a recompensa
2. Penalizar mÃ¡s la lentitud
3. Usar throttle variable

### Para mejor estabilidad
1. Aumentar penalty de colisiÃ³n
2. AÃ±adir penalty gradual por desviaciÃ³n
3. Usar soft update mÃ¡s suave (TAU mÃ¡s pequeÃ±o)

## ğŸ“š Archivos de ConfiguraciÃ³n

### paper_text.txt
Texto completo del paper original

### PHASE_1_COMPLETED.md
DocumentaciÃ³n de implementaciÃ³n fase 1

### DRL_FLATTEN_IMAGE_PAPER_CONFIG.md
Detalles de configuraciÃ³n del paper

## ğŸ“ Referencias

**Paper Original:**
PÃ©rez-Gil, Ã“., Barea, R., LÃ³pez-GuillÃ©n, E., Bergasa, L. M., GÃ³mez-HuÃ©lamo, C., GutiÃ©rrez, R., & DÃ­az-DÃ­az, A. (2022). Deep reinforcement learning based control for Autonomous Vehicles in CARLA. *Multimedia Tools and Applications*, 81(3), 3553-3576.

**CARLA Simulator:**
[carla.org](https://carla.org)

## ğŸ“ Notas Importantes

1. **Guardado automÃ¡tico**: Checkpoints cada 50 episodios
2. **Renders desactivados**: Durante training para velocidad
3. **EvaluaciÃ³n sin exploraciÃ³n**: eps=0 para medir capacidad real
4. **Rutas aleatorias**: GeneralizaciÃ³n mejor que rutas fijas
5. **Paper usa DDPG**: Nosotros implementamos DQN (versiÃ³n discreta)

## âœ… Checklist de ImplementaciÃ³n

- [x] Estado 123-dim (121 imagen + Ï†t + dt)
- [x] Red 2-FC layers (64, 32 neurons)
- [x] 27 acciones discretas
- [x] DQN con experience replay
- [x] Training loop con episodios
- [x] Checkpoints cada 50 episodios
- [x] EvaluaciÃ³n con RMSE
- [x] FunciÃ³n recompensa del paper
- [x] Sensores colisiÃ³n + invasiÃ³n carril
- [x] Rutas aleatorias A* planner
- [x] VisualizaciÃ³n waypoints
- [x] Camera chase mode
- [x] Video recording

## ğŸš€ PrÃ³ximos Pasos

1. Entrenar 500 episodios
2. Evaluar mejor checkpoint (20 iteraciones)
3. Si RMSE > 0.15m â†’ mÃ¡s training
4. Comparar con paper (Tabla 3)
5. Considerar implementar DDPG (mejor rendimiento)
