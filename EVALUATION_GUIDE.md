# üéØ Gu√≠a de Evaluaci√≥n del Agente DRL-Flatten-Image

## üìö Basado en el Paper
**P√©rez-Gil et al. 2022** - Secci√≥n 6.1.2 (Validation Stage)

El paper eval√∫a cada agente conduci√©ndolo 20 veces por la misma ruta y calculando:
- **RMSE** (Root Mean Square Error): Error promedio respecto a trayectoria ideal
- **Error M√°ximo**: Mayor desviaci√≥n en toda la ruta  
- **Tiempo Promedio**: Tiempo en completar la ruta

## üöÄ Uso R√°pido

### 1. Evaluar el √∫ltimo checkpoint entrenado
```bash
python evaluate_agent.py
```

### 2. Evaluar un checkpoint espec√≠fico
```bash
python evaluate_agent.py --checkpoint checkpoints/drl_flatten_episode_200.pth
```

### 3. Evaluar con m√°s iteraciones (mayor precisi√≥n)
```bash
python evaluate_agent.py --iterations 50
```

### 4. Evaluar sin video (m√°s r√°pido)
```bash
python evaluate_agent.py --no-video
```

### 5. Evaluar sin display (headless)
```bash
python evaluate_agent.py --no-display
```

## üìä M√©tricas Calculadas

### Precisi√≥n
- **RMSE promedio**: Distancia promedio de la trayectoria real vs ideal
- **Error m√°ximo**: Mayor desviaci√≥n encontrada
- **Desviaci√≥n est√°ndar**: Consistencia del agente

### Rendimiento
- **Tasa de √©xito**: % de rutas completadas sin colisi√≥n/invasi√≥n
- **Tiempo promedio**: Eficiencia temporal
- **Recompensa total**: M√©trica de aprendizaje

### Comparaci√≥n con Paper (Tabla 3)
```
M√©todo                    RMSE (m)    Max Error (m)
----------------------------------------
LQR (baseline)            0.06        0.74
DQN-Flatten-Image         0.08        -
DDPG-Flatten-Image        0.07        -
Tu modelo                 ???         ???
```

## üéØ Criterios de Calidad

| RMSE       | Evaluaci√≥n                |
|------------|---------------------------|
| < 0.10 m   | üèÜ Excelente              |
| < 0.15 m   | ‚úÖ Bueno                  |
| < 0.20 m   | ‚ö†Ô∏è  Aceptable             |
| > 0.20 m   | ‚ùå Necesita m√°s training  |

## üìÅ Outputs Generados

### 1. Video de evaluaci√≥n
- **Ubicaci√≥n**: `evaluation_output/`
- **Contenido**: Grabaci√≥n de las iteraciones con HUD
- **Formato**: MP4 (creado con ffmpeg)

### 2. Archivo de resultados
- **Nombre**: `evaluation_results_episode_XXX.txt`
- **Contenido**: M√©tricas detalladas y comparaci√≥n con paper

### 3. Console output
- Resumen por iteraci√≥n
- Estad√≠sticas finales
- Comparaci√≥n con benchmarks

## üîß Opciones Avanzadas

```bash
# Evaluaci√≥n completa con display y video
python evaluate_agent.py \
    --checkpoint checkpoints/drl_flatten_final.pth \
    --iterations 20

# Evaluaci√≥n r√°pida sin video
python evaluate_agent.py \
    --checkpoint checkpoints/drl_flatten_episode_100.pth \
    --iterations 5 \
    --no-video

# Evaluaci√≥n headless (servidor sin X)
python evaluate_agent.py \
    --no-display \
    --no-video
```

## üìà Interpretaci√≥n de Resultados

### ‚úÖ Agente Bien Entrenado
```
RMSE promedio: 0.075 ¬± 0.012 m
Error m√°ximo promedio: 0.423 m
Tasa de √©xito: 95.0% (19/20)
Colisiones: 0
Invasiones de carril: 1
```

### ‚ö†Ô∏è Agente Necesita M√°s Training
```
RMSE promedio: 0.234 ¬± 0.089 m
Error m√°ximo promedio: 1.567 m
Tasa de √©xito: 65.0% (13/20)
Colisiones: 4
Invasiones de carril: 3
```

## üêõ Troubleshooting

### Error: "No se encontraron checkpoints"
```bash
# Primero entrena un modelo
python src/main.py
```

### Error: "CARLA no est√° corriendo"
```bash
# En otra terminal, lanza CARLA primero
./launch_carla.sh
```

### Display no funciona (headless)
```bash
# Usa modo headless
python evaluate_agent.py --no-display
```

## üìù Notas Importantes

1. **Modo de Evaluaci√≥n**: El agente NO explora (eps=0), solo explota lo aprendido
2. **Rutas Aleatorias**: Cada iteraci√≥n usa una ruta diferente (como en training)
3. **Sensores**: Usa misma configuraci√≥n que training (11√ó11, œÜt, dt)
4. **Determinismo**: Con el mismo checkpoint, resultados pueden variar por rutas aleatorias

## üéì Siguientes Pasos

1. **Si RMSE > 0.15m**: Continuar training (m√°s episodios)
2. **Si tasa √©xito < 80%**: Revisar funci√≥n de recompensa
3. **Si muchas colisiones**: Aumentar penalizaci√≥n por colisi√≥n
4. **Si buen RMSE pero lento**: Entrenar con recompensa de velocidad

## üìö Referencias

- Paper: P√©rez-Gil, √ì., Barea, R., L√≥pez-Guill√©n, E. et al. (2022). 
  "Deep reinforcement learning based control for Autonomous Vehicles in CARLA"
  *Multimedia Tools and Applications*, 81, 3553‚Äì3576
