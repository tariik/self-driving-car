"""
Test script para verificar la nueva Funci√≥n de Recompensa
Como en el paper de P√©rez-Gil et al. (2022)
"""

import numpy as np
import sys
import os

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '.'))

from src.env.base_env import BASE_EXPERIMENT_CONFIG, BaseEnv
from src.env.carla_env import CarlaEnv

def test_reward_function():
    """Test para verificar la nueva funci√≥n de recompensa"""
    
    print("=" * 70)
    print("üß™ TEST: FUNCI√ìN DE RECOMPENSA (Paper Implementation)")
    print("=" * 70)
    print()
    
    # Configuraci√≥n
    config = {
        "carla": {
            "host": "localhost",
            "port": 3000,
            "timeout": 60.0,
            "timestep": 0.1,
            "retries_on_error": 10,
            "resolution_x": 800,
            "resolution_y": 600,
            "quality_level": "Low",
            "enable_rendering": True,
            "show_display": False,
            "use_external_server": True
        },
        "others": {
            "framestack": 4,
            "max_time_idle": 100,
            "max_time_episode": 500,
        }
    }
    
    # Configuraci√≥n del experimento
    experiment_config = BASE_EXPERIMENT_CONFIG.copy()
    experiment_config["hero"]["sensors"] = {
        "rgb_camera": {
            "type": "sensor.camera.rgb",
            "transform": "1.5,0.0,1.5,0.0,0.0,0.0",
            "image_size_x": "84",
            "image_size_y": "84",
            "fov": "90",
            "size": 84
        }
    }
    experiment_config["hero"]["spawn_points"] = []
    experiment_config["town"] = "Town01"
    experiment_config["weather"] = "ClearNoon"
    experiment_config["others"] = config["others"]
    
    # Crear entorno
    print("üöó Creando entorno CARLA...")
    experiment = BaseEnv(experiment_config)
    env = CarlaEnv(experiment, config)
    
    print("‚úÖ Entorno creado")
    print()
    
    # Reset
    print("üîÑ Reseteando entorno...")
    observation, info = env.reset()
    
    print(f"‚úÖ Reset completado")
    print()
    
    # Verificar funci√≥n de recompensa
    print("üìä VERIFICANDO FUNCI√ìN DE RECOMPENSA:")
    print("-" * 70)
    print()
    
    print("üìñ F√≥rmula del Paper:")
    print("   R = -200  (colisi√≥n o salida de carril)")
    print("   R = |vt¬∑cos(œÜt)| - |vt¬∑sin(œÜt)| - |vt|¬∑|dt|  (conducci√≥n normal)")
    print("   R = +100  (meta alcanzada)")
    print()
    print("   Donde:")
    print("   - vt: velocidad del veh√≠culo (m/s)")
    print("   - dt: distancia al centro del carril (m)")
    print("   - œÜt: √°ngulo respecto al carril (rad)")
    print()
    print("-" * 70)
    print()
    
    # Ejecutar varios steps y analizar recompensas
    print("üéÆ EJECUTANDO STEPS Y ANALIZANDO RECOMPENSAS:")
    print("-" * 70)
    print()
    
    total_reward = 0
    step_rewards = []
    
    for step_num in range(20):
        # Diferentes acciones para probar la funci√≥n de recompensa
        if step_num < 5:
            action = 22  # Throttle 1.0, steering 0 (acelerar recto)
            action_desc = "Acelerar recto"
        elif step_num < 10:
            action = 15  # Throttle 0.6, steering 0 (avanzar moderado)
            action_desc = "Avanzar moderado"
        elif step_num < 15:
            action = 16  # Throttle 0.6, steering 0.75 (girar derecha)
            action_desc = "Girar derecha"
        else:
            action = 19  # Throttle 0.6, steering -0.75 (girar izquierda)
            action_desc = "Girar izquierda"
        
        observation, reward, done, truncated, info = env.step(action)
        
        total_reward += reward
        step_rewards.append(reward)
        
        # Obtener driving features del info
        if 'driving_features' in info:
            df = info['driving_features']
            vt, dt, œÜt = df[0], df[1], df[2]
            
            # Calcular componentes de la recompensa
            r_forward = np.abs(vt * np.cos(œÜt))
            r_lateral = np.abs(vt * np.sin(œÜt))
            r_deviation = np.abs(vt) * np.abs(dt)
            
            print(f"Step {step_num + 1:2d} [{action_desc:16s}]:")
            print(f"   Driving: vt={vt:5.2f} m/s | dt={dt:5.2f} m | œÜt={np.degrees(œÜt):6.1f}¬∞")
            print(f"   Reward components:")
            print(f"      +{r_forward:6.3f} (velocidad adelante)")
            print(f"      -{r_lateral:6.3f} (velocidad lateral)")
            print(f"      -{r_deviation:6.3f} (desviaci√≥n centro)")
            print(f"   ‚Üí Reward total: {reward:7.3f}")
            print()
        else:
            print(f"Step {step_num + 1:2d}: reward={reward:6.2f} (sin driving features)")
            print()
        
        if done or truncated:
            print(f"‚ö†Ô∏è Episodio terminado en step {step_num + 1}")
            if done:
                print("   Raz√≥n: done=True")
            if truncated:
                print("   Raz√≥n: truncated=True")
            break
    
    print("-" * 70)
    print()
    
    # Estad√≠sticas finales
    print("üìà ESTAD√çSTICAS DE RECOMPENSAS:")
    print("-" * 70)
    print(f"   Total acumulada: {total_reward:.2f}")
    print(f"   Promedio:        {np.mean(step_rewards):.3f}")
    print(f"   M√°xima:          {np.max(step_rewards):.3f}")
    print(f"   M√≠nima:          {np.min(step_rewards):.3f}")
    print(f"   Desv. est√°ndar:  {np.std(step_rewards):.3f}")
    print("-" * 70)
    print()
    
    # An√°lisis de comportamiento
    print("üîç AN√ÅLISIS DE COMPORTAMIENTO:")
    print("-" * 70)
    
    positive_rewards = [r for r in step_rewards if r > 0]
    negative_rewards = [r for r in step_rewards if r < 0]
    
    print(f"   Rewards positivas: {len(positive_rewards)}/{len(step_rewards)} steps")
    print(f"   Rewards negativas: {len(negative_rewards)}/{len(step_rewards)} steps")
    
    if positive_rewards:
        print(f"   Promedio positivo: {np.mean(positive_rewards):.3f}")
    if negative_rewards:
        print(f"   Promedio negativo: {np.mean(negative_rewards):.3f}")
    
    print()
    print("üí° Interpretaci√≥n:")
    if total_reward > 0:
        print("   ‚úÖ El agente est√° conduciendo bien (reward total positiva)")
    else:
        print("   ‚ö†Ô∏è  El agente necesita mejorar (reward total negativa)")
    
    avg_reward = np.mean(step_rewards)
    if avg_reward > 0.5:
        print("   ‚úÖ Recompensa promedio alta: conducci√≥n eficiente")
    elif avg_reward > 0:
        print("   ‚ö†Ô∏è  Recompensa promedio baja: hay margen de mejora")
    else:
        print("   ‚ùå Recompensa promedio negativa: conducci√≥n ineficiente")
    
    print("-" * 70)
    print()
    
    # Cerrar entorno
    env.close()
    
    print("=" * 70)
    print("‚úÖ TEST COMPLETADO EXITOSAMENTE")
    print("=" * 70)
    print()
    print("üìñ Comparaci√≥n con Paper:")
    print("   ‚úÖ Funci√≥n de recompensa implementada seg√∫n P√©rez-Gil et al. (2022)")
    print("   ‚úÖ Premia velocidad hacia adelante")
    print("   ‚úÖ Penaliza zigzagueo (velocidad lateral)")
    print("   ‚úÖ Penaliza desviaci√≥n del centro del carril")
    print("   ‚úÖ Penalizaci√≥n fuerte (-200) para colisi√≥n/salida")
    print("   ‚úÖ Recompensa alta (+100) para meta alcanzada")
    print()
    print("üí° Pr√≥ximos pasos:")
    print("   1. Entrenar agente con nueva reward function")
    print("   2. Comparar convergencia vs reward anterior")
    print("   3. Validar RMSE en trayectorias")
    print()
    
    return True

if __name__ == "__main__":
    try:
        success = test_reward_function()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Test interrumpido por usuario")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå ERROR durante el test:")
        print(f"   {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
