# üìã Configuraci√≥n CARLA del Paper de Referencia

**Paper**: "Deep reinforcement learning based control for Autonomous Vehicles in CARLA"  
**Autores**: √ìscar P√©rez-Gil et al. (Universidad de Alcal√°, 2022)  
**DOI**: 10.1007/s11042-021-11437-3

---

## üéØ OBJETIVO DEL EXPERIMENTO

Entrenar agentes DRL (DQN y DDPG) para **navegaci√≥n aut√≥noma** en CARLA:
- Seguir una ruta predeterminada lo m√°s r√°pido posible
- Evitar colisiones y salidas de carril
- Navegar en entorno urbano din√°mico

---

## üñ•Ô∏è CONFIGURACI√ìN T√âCNICA

### Hardware Utilizado
```
CPU: Intel Core i7-9700k
RAM: 32GB
GPU: NVIDIA GeForce RTX 2080 Ti (11GB VRAM)
CUDA: Habilitado
```

### Software
- **CARLA Simulator**: Basado en Unreal Engine 4
- **Framework**: Python API de CARLA
- **RL Frameworks**: M√∫ltiples frameworks open-source
- **Integraci√≥n**: Posible con ROS (CARLA ROS Bridge)

---

## üó∫Ô∏è MAPAS Y RUTAS

### Mapas Utilizados
- **Town01**: Principal mapa de validaci√≥n
  - Ruta de validaci√≥n: ~180 metros
  - Incluye curvas en ambas direcciones
  - Incluye secciones rectas

### Sistema de Rutas
- **Planificador global**: A* basado en waypoints
- **Generaci√≥n din√°mica**: Rutas aleatorias en cada episodio
- **Fuente**: Waypoints provistos por CARLA PythonAPI
- **Formato**: OpenDrive standard para definir carreteras

---

## üéÆ ESPACIO DE ESTADOS (State Space)

El estado `S` se modela como tupla `st = (vft, dft)`:

### 1. Visual Features Vector (vft)
Depende del agente espec√≠fico (ver secci√≥n "Agentes")

### 2. Driving Features Vector (dft)
```python
dft = (vt, dt, œÜt)
```
- **vt**: Velocidad del veh√≠culo (m/s)
- **dt**: Distancia al centro del carril (m)
- **œÜt**: √Ångulo respecto al centro del carril (radianes)

**Fuente**: Estos datos son provistos directamente por CARLA Simulator

---

## üïπÔ∏è ESPACIO DE ACCIONES (Action Space)

### DQN (Discreto)
- **Total acciones**: 27 comandos discretos
- **Steering**: 9 posiciones [-1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1]
- **Throttle**: 3 posiciones [0, 0.5, 1]
- **Brake**: No implementado (freno regenerativo del veh√≠culo es suficiente)

### DDPG (Continuo) ‚≠ê MEJOR
- **Steering**: Continuo [-1, 1]
- **Throttle**: Continuo [0, 1]
- **Brake**: No implementado
- **Ventaja**: Control m√°s suave y realista

---

## üéÅ FUNCI√ìN DE RECOMPENSA (Reward Function)

```python
# Colisi√≥n, cambio de carril o salida de carretera
R = -200

# Mientras el carro est√° en el carril
R = Œ£ |vt¬∑cos(œÜt)| - |vt¬∑sin(œÜt)| - |vt|¬∑|dt|

# Meta alcanzada
R = +100
```

### Componentes de la Recompensa
- **+|vt¬∑cos(œÜt)|**: Premia velocidad longitudinal (avanzar)
- **-|vt¬∑sin(œÜt)|**: Penaliza velocidad transversal (zigzaguear)
- **-|vt|¬∑|dt|**: Penaliza desviaci√≥n del centro del carril
- **-200**: Penalizaci√≥n fuerte por colisi√≥n/salida
- **+100**: Recompensa por completar ruta

---

## üì∑ SENSORES UTILIZADOS

### 1. C√°mara RGB
- **Resoluci√≥n original**: 640x480 p√≠xeles
- **Procesamiento**: Var√≠a seg√∫n agente (ver "Agentes")
- **Ubicaci√≥n**: Vista frontal del veh√≠culo
- **Uso**: Extracci√≥n de caracter√≠sticas visuales de la carretera

### 2. Sensor de Invasi√≥n de Carril (Lane Invasor)
- **Prop√≥sito**: Detectar salidas de carril
- **Acci√≥n**: Termina episodio si se activa

### 3. Sensor de Colisi√≥n (Collision Sensor)
- **Prop√≥sito**: Detectar colisiones
- **Acci√≥n**: Termina episodio si se activa

### 4. Waypoints
- **Fuente**: A* global planner de CARLA
- **Formato**: Lista de puntos (x, y, z) en coordenadas globales
- **Transformaci√≥n**: Convertidos a coordenadas locales del veh√≠culo

```python
# Matriz de transformaci√≥n (rotaci√≥n + traslaci√≥n)
M = [[cos(œÜc), -sin(œÜc), 0, Xc],
     [sin(œÜc),  cos(œÜc), 0, Yc],
     [0,        0,       1, Zc],
     [0,        0,       0, 1]]
```
- **(Xc, Yc, Zc)**: Posici√≥n global del veh√≠culo
- **œÜc**: Heading/yaw del veh√≠culo

---

## ü§ñ AGENTES PROPUESTOS (4 Arquitecturas)

### 1. DRL-Flatten-Image Agent
**Entrada**: Imagen B/N segmentada de la carretera
- **Procesamiento**: 640x480 ‚Üí 11x11 (reducci√≥n de 300k a 121 datos)
- **Flatten**: Vector de 121 componentes
- **Estado**: `S = ([P‚ÇÄ, P‚ÇÅ, ..., P‚ÇÅ‚ÇÇ‚ÇÄ], œÜt, dt)`
- **Red**: 2 capas Fully-Connected
- **Ventaja**: Muy simple y r√°pido

### 2. DRL-Carla-Waypoints Agent ‚≠ê MEJOR DDPG
**Entrada**: Waypoints directamente de CARLA
- **Cantidad**: 15 waypoints (ventana FIFO)
- **Referencia**: Transformados a coordenadas locales del veh√≠culo
- **Coordenadas usadas**: Solo X (posici√≥n lateral en el carril)
- **Estado**: `S = ([wp‚ÇÄ...wp‚ÇÅ‚ÇÑ], œÜt, dt)`
- **Red**: 2 capas Fully-Connected
- **RMSE**: 0.10m (mejor resultado DDPG)

### 3. DRL-CNN Agent
**Entrada**: Imagen RGB 640x480
- **CNN**: 3 capas convolucionales
  - Conv1: 64 filtros [7x7] + ReLU + AvgPool
  - Conv2: 64 filtros [5x5] + ReLU + AvgPool
  - Conv3: 64 filtros [3x3] + ReLU + AvgPool
- **Flatten**: Salida CNN aplanada
- **Estado**: `S = ([It], œÜt, dt)`
- **Red**: CNN + 2 capas Fully-Connected
- **Desventaja**: 307,200 datos ‚Üí entrenamiento muy lento

### 4. DRL-Pre-CNN Agent
**Entrada**: Imagen RGB 640x480
- **CNN Pre-entrenado**: Offline con dataset de im√°genes-waypoints
- **Funci√≥n**: Predecir waypoints desde imagen
- **Estado real**: `S = ([wp‚ÇÄ...wp‚ÇÅ‚ÇÑ], œÜt, dt)` (waypoints predichos)
- **Red**: CNN (pre-entrenado) + 2 capas Fully-Connected
- **RMSE DDPG**: 0.115m (segundo mejor)

---

## üèãÔ∏è PROCESO DE ENTRENAMIENTO

### Workflow de Entrenamiento

```python
for episode in range(M_episodes):
    # 1. Inicializar episodio
    route = a_star_planner.get_random_route()  # Ruta aleatoria
    
    for step in range(T_steps):
        # 2. Observar estado
        state = get_observation()  # S = ([D], œÜt, dt)
        
        # 3. Predecir acci√≥n
        action = drl_agent.predict(state)  # (throttle, steering)
        
        # 4. Ejecutar acci√≥n en CARLA
        carla.apply_control(action)
        
        # 5. Calcular recompensa
        reward = calculate_reward()
        
        # 6. Verificar sensores
        if lane_invasor.triggered or collision_sensor.triggered:
            episode.end()
            vehicle.reset()  # Reposicionar en centro del carril
            break
        
        # 7. Entrenar agente
        drl_agent.train(state, action, reward, next_state)
```

### Par√°metros de Entrenamiento

#### DQN
- **Episodios totales**: 20,000 - 120,000 (seg√∫n agente)
- **Mejor episodio**: 8,300 - 108,600
- **Criterio**: Mayor recompensa acumulada + mayor distancia recorrida
- **Tiempo**: Mucho m√°s largo que DDPG

#### DDPG ‚≠ê MEJOR
- **Episodios totales**: 500 - 60,000 (seg√∫n agente)
- **Mejor episodio**: 50 - 45,950
- **Convergencia**: Modelos tempranos (< 200 episodios)
- **Tiempo**: Dr√°sticamente reducido vs DQN

---

## üìä VALIDACI√ìN Y M√âTRICAS

### Proceso de Validaci√≥n

1. **Ruta fija**: Seleccionar ruta espec√≠fica en el mapa
2. **Iteraciones**: Conducir 20 veces la misma ruta
3. **Ground Truth**: Ruta ideal obtenida interpolando waypoints de CARLA
4. **Comparaci√≥n**: vs LQR controller (m√©todo cl√°sico)

### M√©tricas Calculadas

#### RMSE (Root Mean Square Error)
```python
RMSE = ‚àö(Œ£(posici√≥n_real - posici√≥n_ideal)¬≤ / N)
```

#### Error M√°ximo
```python
max_error = max(|posici√≥n_real - posici√≥n_ideal|)
```

#### Tiempo Promedio
Tiempo promedio en completar la ruta

### Resultados Town01 (180m)

| M√©todo | RMSE (m) | Error M√°x (m) | Tiempo (s) |
|--------|----------|---------------|------------|
| **LQR (Cl√°sico)** | 0.06 | 0.74 | 17.4 |
| Manual Control | 0.40 | 1.80 | 22.7 |
| **DDPG-Waypoints** ‚≠ê | **0.13** | **1.50** | **20.6** |
| **DDPG-Pre-CNN** | **0.10** | **1.41** | **23.8** |
| DDPG-Flatten | 0.15 | 1.43 | 19.9 |
| DDPG-CNN | 0.75 | 2.55 | 34.2 |
| DQN-Waypoints | 0.21 | 1.32 | 29.3 |
| DQN-Flatten | 0.64 | 3.15 | 27.3 |
| DQN-CNN | 0.83 | 2.15 | 33.3 |

### Resultados 20 Rutas Variadas (180-700m)

| M√©todo | RMSE (m) | Error M√°x (m) | Tiempo (s) |
|--------|----------|---------------|------------|
| **LQR (Cl√°sico)** | 0.095 | 1.305 | 65.60 |
| **DDPG-Waypoints** ‚≠ê | **0.10** | **1.46** | **62.25** |
| **DDPG-Pre-CNN** | **0.115** | **1.512** | **65.12** |
| DDPG-Flatten | 0.134 | 1.522 | 63.97 |
| DDPG-CNN | 0.67 | 2.78 | 125.43 |

---

## üîë CONCLUSIONES CLAVE DEL PAPER

### ‚úÖ DDPG vs DQN
- **DDPG es SUPERIOR**: Control continuo m√°s realista
- **DQN limitado**: Naturaleza discreta dificulta el control suave
- **Reducci√≥n de tiempo**: DDPG entrena ~50x m√°s r√°pido

### ‚≠ê Mejor Agente: DDPG-Carla-Waypoints
- **RMSE**: 0.10m (pr√°cticamente igual a LQR: 0.095m)
- **Ventaja sobre cl√°sicos**: No requiere tuning manual complejo
- **Reproducibilidad**: F√°cil replicaci√≥n en cualquier entorno
- **Sin conocimiento experto**: No necesita teor√≠a de control electr√≥nico

### üéØ Agentes Recomendados (en orden)
1. **DDPG-Carla-Waypoints**: Mejor performance (RMSE 0.10m)
2. **DDPG-Pre-CNN**: Segundo mejor (RMSE 0.115m)
3. **DDPG-Flatten-Image**: Balance velocidad/precisi√≥n (RMSE 0.134m)
4. **DQN-Waypoints**: Si necesitas acciones discretas (RMSE 0.21m)

### ‚ö†Ô∏è Evitar
- **DRL-CNN** (ambos algoritmos): Muy lento, peores resultados

---

## üöÄ CARACTER√çSTICAS CARLA UTILIZADAS

### Ventajas de CARLA para este proyecto

1. **Seguridad**: Pruebas ilimitadas sin riesgo
2. **Ground Truth**: Odometr√≠a real + ruta ideal disponibles
3. **Flexibilidad**: Control total de clima, peatones, tr√°fico, sensores
4. **Fast Simulation**: Modo sin render para entrenamiento r√°pido
5. **Realismo**: Apariencia hiper-realista (Unreal Engine 4)
6. **Escenarios**: Scenario Runner para construir casos de prueba
7. **API Python**: Control program√°tico completo
8. **Gratuito**: Open-source

### Configuraci√≥n CARLA Utilizada

```python
# Modo de ejecuci√≥n
- Fast simulation: Render deshabilitado para entrenamiento
- Normal mode: Con render para validaci√≥n visual

# Control de simulaci√≥n
- Physics: GPU dedicada recomendada
- Client-side: Control de l√≥gica de actores
- Server-side: C√°lculo de f√≠sica

# Integraci√≥n
- Python API: Comunicaci√≥n directa
- ROS Bridge: Integraci√≥n futura con veh√≠culo real
```

---

## üìù APLICACI√ìN A TU PROYECTO

### Tu Configuraci√≥n Actual (Recomendaciones)

```python
# ‚úÖ BIEN - Configuraciones que coinciden con el paper
- GPU: RTX 3080 (10GB) - Similar a RTX 2080 Ti (11GB)
- CUDA: Habilitado
- Framework: Python + Gymnasium
- Algoritmo: DQN (considerado en el paper)

# ‚ö†Ô∏è DIFERENCIAS - Ajustes recomendados
- C√°mara: Actualmente frontal (1.5, 0, 1.5, 0¬∞, 0¬∞, 0¬∞) ‚úÖ CORRECTO
- Resoluci√≥n: 84x84 grayscale - Paper usa 640x480 RGB
- Estado: Frame stack (4 frames) - Paper usa caracter√≠sticas de conducci√≥n
- Acciones: 29 clases discretas - Paper usa 27 clases DQN

# üéØ RECOMENDACI√ìN: Implementar DDPG
- Mejor performance demostrada
- Entrenamiento m√°s r√°pido
- Control continuo m√°s realista
- Tranferencia a veh√≠culo real m√°s f√°cil
```

### Mejoras Sugeridas Basadas en el Paper

1. **Algoritmo**: Cambiar de DQN a DDPG
2. **Estado**: Agregar driving features (vt, dt, œÜt) de CARLA
3. **Waypoints**: Usar planificador A* de CARLA (como DDPG-Waypoints)
4. **Sensores**: Agregar lane_invasor y collision_sensor
5. **Recompensa**: Implementar funci√≥n de recompensa del paper
6. **Validaci√≥n**: Comparar con ground truth de waypoints de CARLA
7. **Rutas**: Generar rutas aleatorias en cada episodio

---

## üìö REFERENCIAS

- Paper original: https://doi.org/10.1007/s11042-021-11437-3
- CARLA Simulator: https://carla.org
- Unreal Engine 4: https://www.unrealengine.com
- OpenDrive: https://www.asam.net/standards/detail/opendrive/

---

**Fecha de an√°lisis**: Octubre 2025  
**Preparado para**: Proyecto self-driving-car CARLA DRL
