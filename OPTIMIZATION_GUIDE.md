# üöÄ Gu√≠a de Optimizaci√≥n para Entrenamiento DRL

## üìä An√°lisis del C√≥digo Actual

### Puntos Cr√≠ticos Identificados

1. **Logs en consola cada 10 steps** ‚Üí Ralentiza el entrenamiento
2. **Display activo** ‚Üí Consume CPU/GPU para renderizado
3. **C√°lculo de velocidad en cada step** ‚Üí Llamadas repetidas a CARLA
4. **Extracci√≥n de caracter√≠sticas repetida** ‚Üí C√≥digo duplicado
5. **Sincronizaci√≥n de display** ‚Üí Overhead de pygame

## ‚ö° Optimizaciones Recomendadas

### 1. üéØ Reducir Frecuencia de Logs (Alto Impacto)

**Problema:** Imprimir en consola cada 10 steps es costoso.

**Soluci√≥n:**

```python
# En src/main.py
LOG_FREQUENCY = 100  # Cambiar de 10 a 100 o m√°s

if step % LOG_FREQUENCY == 0:
    # ... logs ...
```

**Impacto:** +10-15% velocidad de entrenamiento

---

### 2. üñ•Ô∏è Desactivar Display en Training (Muy Alto Impacto)

**Problema:** El display consume recursos gr√°ficos y CPU.

**Soluci√≥n:**

```python
# En src/main.py, l√≠nea ~95
use_display = False  # ‚ö° DESACTIVAR para m√°ximo rendimiento
```

**Impacto:** +30-40% velocidad de entrenamiento

---

### 3. üîß Cachear C√°lculos Repetidos (Medio Impacto)

**Problema:** Extraemos caracter√≠sticas del estado varias veces.

**Soluci√≥n:** Crear funci√≥n helper

```python
def extract_state_info(state, env):
    """Extrae toda la informaci√≥n del estado de una vez"""
    phi_t = state[121] if len(state) > 121 else 0.0
    d_t = state[122] if len(state) > 122 else 0.0
    
    v_t = np.linalg.norm([
        env.hero.get_velocity().x,
        env.hero.get_velocity().y,
        env.hero.get_velocity().z
    ]) if hasattr(env, 'hero') else 0.0
    
    return v_t, d_t, phi_t
```

**Impacto:** +5-8% velocidad de entrenamiento

---

### 4. ‚è±Ô∏è Optimizar Timestep de CARLA (Medio Impacto)

**Problema:** Timestep de 0.1s puede ser demasiado fino.

**Soluci√≥n:**

```python
# En src/main.py, l√≠nea ~23
"timestep": 0.05,  # Cambiar de 0.1 a 0.05 (duplica velocidad de simulaci√≥n)
```

**Impacto:** +50% velocidad de simulaci√≥n (pero menos realista)

**‚ö†Ô∏è Cuidado:** Afecta la f√≠sica del simulador

---

### 5. üéÆ Modo Sin Renderizado (Muy Alto Impacto)

**Problema:** CARLA renderiza aunque no uses las im√°genes RGB para display.

**Soluci√≥n:**

```python
# En src/main.py
config = {
    "carla": {
        # ...
        "quality_level": "Low",
        "enable_rendering": False,  # ‚ö° Desactivar si no necesitas visualizaci√≥n
    }
}
```

**Impacto:** +20-30% velocidad

**‚ö†Ô∏è Solo si no necesitas las im√°genes RGB**

---

### 6. üìâ Reducir Resoluci√≥n de C√°mara (‚ùå NO RECOMENDADO)

**Problema:** Capturamos 640√ó480 y luego reducimos a 11√ó11.

**‚ùå NO APLICAR ESTA OPTIMIZACI√ìN:**

```python
# ‚ùå NO HACER ESTO - Contradice el paper
"image_size_x": "320",  # ‚ùå NO cambiar
"image_size_y": "240",  # ‚ùå NO cambiar
```

**‚ö†Ô∏è RAZ√ìN:** El paper P√©rez-Gil et al. (2022) espec√≠ficamente dice:

> *"from 640x480 pixels to 11x11, reducing the amount of data from 300k to 121"*

**Mantener configuraci√≥n del paper:**
```python
# ‚úÖ CORRECTO - Seg√∫n el paper
"image_size_x": "640",  # ‚úÖ Mantener 640√ó480
"image_size_y": "480",  # ‚úÖ Como especifica el paper
```

**Explicaci√≥n:** 
- El paper captura **640√ó480** intencionalmente
- El **resize a 11√ó11** se hace despu√©s en `post_process_image()`
- Cambiar la resoluci√≥n inicial podr√≠a afectar:
  - La calidad de la informaci√≥n capturada
  - El proceso de resize (interpolaci√≥n)
  - La reproducibilidad del paper
  - Los resultados del entrenamiento

**Impacto potencial si se cambia:** +5-10% velocidad, pero **resultados no validados**

**Conclusi√≥n:** ‚ùå **NO aplicar** - Mantener fidelidad al paper

---

### 7. üóëÔ∏è Desactivar Video Recording (Medio Impacto)

**Problema:** El video recorder guarda frames aunque no crees el video.

**Soluci√≥n:**

```python
# Ya est√° implementado correctamente
SAVE_VIDEO = False  # ‚úÖ Ya est√° desactivado
SAVE_RENDERS = False  # ‚úÖ Ya est√° desactivado
```

**Impacto:** +10-15% velocidad (ya optimizado)

---

### 8. üßÆ Modo As√≠ncrono de CARLA (Alto Impacto)

**Problema:** Modo s√≠ncrono espera cada frame.

**Soluci√≥n:**

```python
# En src/env/carla_core.py, buscar set_synchronous_mode
# Cambiar a modo as√≠ncrono (m√°s r√°pido pero menos determinista)
world.set_synchronous_mode(False)
```

**Impacto:** +40-60% velocidad

**‚ö†Ô∏è Cuidado:** Menos determinista, puede afectar reproducibilidad

---

### 9. üìä Batch de Episodios (Bajo Impacto Inmediato)

**Problema:** Checkpoints y estad√≠sticas cada episodio.

**Soluci√≥n:**

```python
# Guardar checkpoints menos frecuentemente
SAVE_EVERY_N_EPISODES = 100  # Cambiar de 50 a 100

# Estad√≠sticas menos frecuentes
if (episode + 1) % 50 == 0:  # Cambiar de 10 a 50
    # ... print stats ...
```

**Impacto:** +2-3% velocidad

---

### 10. üî¢ NumPy Optimizations (Bajo Impacto)

**Problema:** Operaciones NumPy no optimizadas.

**Soluci√≥n:**

```python
# Usar operaciones vectorizadas
velocity_vector = env.hero.get_velocity()
v_t = np.sqrt(velocity_vector.x**2 + velocity_vector.y**2 + velocity_vector.z**2)

# En lugar de:
v_t = np.linalg.norm([velocity_vector.x, velocity_vector.y, velocity_vector.z])
```

**Impacto:** +1-2% velocidad

---

## üéØ Configuraci√≥n Recomendada para Entrenamiento R√°pido

### Archivo: `src/main.py` (Optimizado)

```python
# ========== CONFIGURACI√ìN DE ENTRENAMIENTO OPTIMIZADO ==========

# Display y logging
use_display = False  # ‚ö° DESACTIVADO para m√°ximo rendimiento
LOG_FREQUENCY = 100  # Logs cada 100 steps en lugar de 10

# Guardado de datos
SAVE_RENDERS = False  # ‚úÖ Desactivado
SAVE_VIDEO = False    # ‚úÖ Desactivado
DEBUG_STATE_FIRST_3_STEPS = False  # ‚úÖ Desactivado

# CARLA settings
config = {
    "carla": {
        "timestep": 0.05,  # ‚ö° Simulaci√≥n m√°s r√°pida (era 0.1)
        "quality_level": "Low",
        "enable_rendering": True,  # Necesario para la c√°mara del agente
    }
}

# Checkpoints menos frecuentes
SAVE_EVERY_N_EPISODES = 100  # Era 50

# Estad√≠sticas menos frecuentes  
if (episode + 1) % 50 == 0:  # Era 10
    # ... print stats ...
```

### Estimaci√≥n de Mejora

| Optimizaci√≥n | Impacto | Acumulado | Recomendado |
|--------------|---------|-----------|-------------|
| Base (sin cambios) | 100% | 100% | - |
| + Desactivar display | +35% | 135% | ‚úÖ S√≠ |
| + Logs cada 100 steps | +12% | 152% | ‚úÖ S√≠ |
| + Cachear c√°lculos | +7% | 163% | ‚úÖ S√≠ |
| + Timestep 0.05 | +50% | 244% | ‚ö†Ô∏è Opcional |
| ~~Reducir resoluci√≥n~~ | ~~+8%~~ | - | ‚ùå No (contradice paper) |
| **TOTAL SEGURO** | **+63%** | **~163%** | **‚úÖ Sin alterar paper** |

---

## üìà Configuraci√≥n por Fase de Entrenamiento

### Fase 1: Entrenamiento R√°pido (Exploraci√≥n)

```python
use_display = False
LOG_FREQUENCY = 200
timestep = 0.05
SAVE_EVERY_N_EPISODES = 100
MAX_EPISODES = 500
```

**Objetivo:** Entrenar r√°pido para probar hiperpar√°metros

---

### Fase 2: Entrenamiento Normal (Producci√≥n)

```python
use_display = False
LOG_FREQUENCY = 100
timestep = 0.1
SAVE_EVERY_N_EPISODES = 50
MAX_EPISODES = 500
```

**Objetivo:** Entrenamiento estable y reproducible

---

### Fase 3: Debugging/Visualizaci√≥n

```python
use_display = True
LOG_FREQUENCY = 10
timestep = 0.1
SAVE_EVERY_N_EPISODES = 10
MAX_EPISODES = 5  # Pocos episodios
```

**Objetivo:** Ver qu√© hace el agente, debugging

---

## üîß Script de Aplicaci√≥n Autom√°tica

Puedes crear un script para cambiar entre modos:

```bash
# fast_train.sh
#!/bin/bash
# Configura para entrenamiento r√°pido
sed -i 's/use_display = True/use_display = False/' src/main.py
sed -i 's/LOG_FREQUENCY = 10/LOG_FREQUENCY = 100/' src/main.py
sed -i 's/timestep": 0.1/timestep": 0.05/' src/main.py
python src/main.py
```

```bash
# debug_train.sh
#!/bin/bash
# Configura para debugging
sed -i 's/use_display = False/use_display = True/' src/main.py
sed -i 's/LOG_FREQUENCY = 100/LOG_FREQUENCY = 10/' src/main.py
sed -i 's/timestep": 0.05/timestep": 0.1/' src/main.py
python src/main.py
```

---

## üìä Monitoreo de Rendimiento

### Agregar Timer

```python
import time

# Al inicio del entrenamiento
training_start = time.time()

# Al final de cada episodio
episode_time = time.time() - episode_start
print(f"   ‚è±Ô∏è  Episode time: {episode_time:.1f}s")

# Al final del entrenamiento
training_time = time.time() - training_start
print(f"‚è±Ô∏è  Total training time: {training_time/60:.1f} minutes")
print(f"‚è±Ô∏è  Average per episode: {training_time/MAX_EPISODES:.1f}s")
```

---

## ‚ö†Ô∏è Advertencias Importantes

### Optimizaciones Seguras (No afectan reproducibilidad del paper)

‚úÖ **Desactivar display** ‚Üí Solo visualizaci√≥n, no afecta entrenamiento
‚úÖ **Reducir frecuencia de logs** ‚Üí Solo output, no afecta algoritmo
‚úÖ **Cachear c√°lculos** ‚Üí Mismos resultados, m√°s eficiente
‚úÖ **Guardar checkpoints menos frecuentemente** ‚Üí Solo I/O

### Optimizaciones Riesgosas (Pueden afectar resultados)

‚ö†Ô∏è **Timestep m√°s peque√±o** ‚Üí F√≠sica menos realista, resultados diferentes
‚ö†Ô∏è **Modo as√≠ncrono** ‚Üí Menos reproducibilidad, no determinista
‚ùå **Cambiar resoluci√≥n de c√°mara** ‚Üí **Contradice el paper directamente**
‚ùå **Desactivar renderizado** ‚Üí Necesario para capturar im√°genes

### Regla de Oro

> **Si el paper especifica un valor, NO lo cambies a menos que sea solo para visualizaci√≥n/logging**

Par√°metros del paper que **NO se deben cambiar**:
- ‚ùå **Resoluci√≥n de c√°mara: 640√ó480** (Paper: "from 640x480 pixels to 11x11")
- ‚ùå **Tama√±o final de imagen: 11√ó11** (Paper: "reducing the amount of data from 300k to 121")
- ‚ùå **Frame stack: 1** (No frame stacking en DRL-Flatten-Image)
- ‚ùå **N√∫mero de episodios DDPG: ~500** (Paper Table 2: "DDPG-Flatten-Image: 500 episodes, Best: 50")
- ‚ùå **N√∫mero de acciones: 27** (Paper: "27 discrete driving actions")
- ‚ùå **Estado: 123 dimensiones** (Paper Eq. 21: "S = ([Pt0...Pt120], œÜt, dt)" = 121 + 2)
- ‚ùå **Red: 2 Fully-Connected Layers** (Paper: "really simple 2 Fully-Connected Layers network")

Par√°metros que **S√ç se pueden optimizar**:
- ‚úÖ Display on/off
- ‚úÖ Frecuencia de logs
- ‚úÖ Frecuencia de checkpoints
- ‚úÖ Video recording on/off

---

## ‚úÖ Checklist de Optimizaci√≥n

Antes de entrenar 500 episodios completos:

- [ ] `use_display = False`
- [ ] `LOG_FREQUENCY = 100` o m√°s
- [ ] `SAVE_VIDEO = False`
- [ ] `SAVE_RENDERS = False`
- [ ] Considerar `timestep = 0.05` (opcional)
- [ ] `SAVE_EVERY_N_EPISODES = 100` (opcional)
- [ ] Cerrar otras aplicaciones pesadas
- [ ] Verificar que CARLA corra solo (sin display)

---

## üéØ Pr√≥ximos Pasos

1. **Aplicar optimizaciones b√°sicas** (display, logs)
2. **Ejecutar 50 episodios de prueba** para medir tiempo
3. **Ajustar configuraci√≥n** seg√∫n resultados
4. **Entrenar modelo completo** (500 episodios)
5. **Evaluar con display activo** usando `evaluate_agent.py`

